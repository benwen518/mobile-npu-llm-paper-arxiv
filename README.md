# Mobile NPU LLM Papers Daily

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

For more carefully curated articles, you can refer to this [repository](https://github.com/benwen518/Mobile_NPU_LLM_Papers).

## Updated on 2025.01.16

![Monthly Trend](imgs/trend.png)

## mobile_npu_llm

|Date|Title|label|Abstract|PDF|Code|
|---|---|---|---|---|---|

## mobile_optimization

|Date|Title|label|Abstract|PDF|Code|
|---|---|---|---|---|---|

## edge_inference

|Date|Title|label|Abstract|PDF|Code|
|---|---|---|---|---|---|

<p align=right>(<a href=#Updated-on-20250116>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/benwen518/mobile-npu-llm-paper-arxiv.svg?style=for-the-badge
[contributors-url]: https://github.com/benwen518/mobile-npu-llm-paper-arxiv/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/benwen518/mobile-npu-llm-paper-arxiv.svg?style=for-the-badge
[forks-url]: https://github.com/benwen518/mobile-npu-llm-paper-arxiv/network/members
[stars-shield]: https://img.shields.io/github/stars/benwen518/mobile-npu-llm-paper-arxiv.svg?style=for-the-badge
[stars-url]: https://github.com/benwen518/mobile-npu-llm-paper-arxiv/stargazers
[issues-shield]: https://img.shields.io/github/issues/benwen518/mobile-npu-llm-paper-arxiv.svg?style=for-the-badge
[issues-url]: https://github.com/benwen518/mobile-npu-llm-paper-arxiv/issues

## 项目介绍

本项目专注于收集和整理arXiv上关于**移动端NPU大语言模型**的最新研究论文，包括：

- **Mobile NPU LLM**: 移动端神经处理单元上的大语言模型
- **Mobile Optimization**: 移动端LLM优化技术（量化、剪枝、压缩等）
- **Edge Inference**: 边缘推理和部署加速

## 主要研究方向

1. **移动端NPU架构优化**
   - 针对移动端NPU的LLM架构设计
   - 内存和计算优化
   - 功耗优化

2. **模型压缩与优化**
   - 量化技术（INT8、FP16等）
   - 知识蒸馏
   - 模型剪枝
   - 动态推理

3. **边缘部署与推理**
   - 边缘设备上的LLM部署
   - 推理加速技术
   - 分布式推理

## 使用方法

运行 `python daily_arxiv.py` 来更新论文数据。

## 贡献

欢迎提交Issue和Pull Request来改进本项目！